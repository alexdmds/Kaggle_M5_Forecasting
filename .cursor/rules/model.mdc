---
description: 
globs: 
alwaysApply: true
---
Tu es un assistant Python expert en s√©ries temporelles, Kaggle et optimisation m√©moire.
G√©n√®re un code d‚Äôentra√Ænement clair, modulaire et efficace pour le challenge Kaggle M5 Forecasting - Accuracy, en suivant les consignes suivantes :

‚∏ª

üß± üîß Structure g√©n√©rale attendue du pipeline
	1.	Chargement des donn√©es
	‚Ä¢	Charger les fichiers : sales_train_validation.csv, calendar.csv, sell_prices.csv
	‚Ä¢	Convertir les colonnes au format category quand pertinent
	‚Ä¢	Appliquer une r√©duction de m√©moire : conversion explicite des types (int16, float32, etc.)
	‚Ä¢	Transformer les donn√©es de ventes de format large √† long (pd.melt sur d_1 √† d_1913)
	2.	Fusion des sources
	‚Ä¢	Fusionner les ventes avec calendar (via d), et avec sell_prices (via store_id, item_id, wm_yr_wk)
	3.	Feature engineering
	‚Ä¢	Pour chaque ligne (produit, magasin, date), cr√©er :
	‚Ä¢	Lags : lag_28, lag_56, √©ventuellement lag_84
	‚Ä¢	Rolling windows : rolling_mean_7, rolling_std_28, rolling_max_7, etc.
	‚Ä¢	Features calendaires : weekday, month, event_name_1, snap_CA, etc.
	‚Ä¢	Prix : sell_price, price_change_from_7d, rolling_mean_price_28
	‚Ä¢	Colonne horizon (de 1 √† 28) = nombre de jours dans le futur pour la pr√©diction
	‚Ä¢	(Optionnel) Indicateurs de lancement du produit (date de d√©but des ventes)
	4.	Cr√©ation du dataset d‚Äôentra√Ænement
	‚Ä¢	Pour chaque s√©rie (id), g√©n√©rer des exemples (t, h) o√π la target = ventes √† t+h, et les features sont ceux disponibles √† t
	‚Ä¢	Filtrer les lignes trop anciennes ou trop √©loign√©es si n√©cessaire
	‚Ä¢	Utiliser seulement les 2 √† 3 derni√®res ann√©es de donn√©es (limiter l‚Äôhistorique pour garder les patterns r√©cents et r√©duire le volume)
	‚Ä¢	Supprimer les lignes avec NaN dans les lags/rolling
	5.	Validation
	‚Ä¢	Split temporel : utiliser les jours d_1886 √† d_1913 comme validation set (correspondant √† l‚Äôhorizon de 28 jours)
	‚Ä¢	(Optionnel) Validation crois√©e glissante sur plusieurs p√©riodes (ex: TimeSeriesSplit)
	6.	Mod√®le LightGBM
	‚Ä¢	Utiliser un mod√®le LightGBM Regressor
	‚Ä¢	Configuration :
	‚Ä¢	objective='tweedie' ou 'poisson'
	‚Ä¢	tweedie_variance_power=1.2
	‚Ä¢	num_leaves, max_depth, min_data_in_leaf, learning_rate, etc.
	‚Ä¢	Early stopping sur un score RMSE
	‚Ä¢	Afficher l‚Äôimportance des features apr√®s entra√Ænement
	‚Ä¢	Sauvegarder le mod√®le (optionnel)
	7.	Logging / Monitoring
	‚Ä¢	Afficher les m√©triques de validation : RMSE, WRMSSE si impl√©ment√©
	‚Ä¢	Logguer les hyperparam√®tres utilis√©s

‚∏ª

‚ö†Ô∏è Contraintes et bonnes pratiques
	‚Ä¢	Code en Python 3, avec Pandas, LightGBM, NumPy
	‚Ä¢	Suivre une structure modulaire (fonctions s√©par√©es : chargement, feature engineering, entra√Ænement)
	‚Ä¢	Ajouter des commentaires clairs pour chaque √©tape
	‚Ä¢	Optimiser l‚Äôusage m√©moire et CPU : √©viter les copies inutiles, prioriser merge plut√¥t que join
	‚Ä¢	Ne pas coder la partie pr√©diction ici (elle sera g√©r√©e plus tard, avec r√©cursivit√©)